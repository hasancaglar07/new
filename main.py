# main.py
# Versiyon 3.4 - Stabil Stream ve Lifespan Manager ile audio streaming gÃ¼ncellemesi. Sunucu Ã¶nbelleklemesini Ã¶nlemek iÃ§in /analysis_history endpoint'ine Cache-Control baÅŸlÄ±ÄŸÄ± eklendi.
import logging
import os
import io
import json
import re
import asyncio
import urllib.parse
from pathlib import Path
from typing import List, Optional
import tempfile
import time
import fitz
import requests
import yt_dlp
from openai import AsyncOpenAI
from dotenv import load_dotenv
import httpx
from fastapi.responses import StreamingResponse
from data.audio_db import get_all_audio_by_source, search_audio_chapters
from data.audio_db import get_audio_path_by_id
from fastapi import FastAPI, HTTPException, Query, Depends, BackgroundTasks, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from whoosh.index import open_dir, Index
from whoosh.qparser import MultifieldParser, AndGroup, QueryParser
from whoosh.searching import Searcher
# from deepgram import DeepgramClient, PrerecordedOptions  # Åimdilik kaldÄ±rÄ±ldÄ±
from data.db import init_db, update_task, get_task, get_all_completed_analyses
from data.articles_db import get_all_articles_by_category, get_article_by_id
from contextlib import asynccontextmanager
# --- Kurulum ve Global YapÄ±landÄ±rma ---
load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Lifespan Manager: Uygulama Ã¶mrÃ¼ boyunca yaÅŸayacak nesneler ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Uygulama baÅŸladÄ±ÄŸÄ±nda:
    # Tek bir httpx.AsyncClient oluÅŸtur ve app state'ine ata.
    app.state.httpx_client = httpx.AsyncClient()
    logger.info("httpx client baÅŸlatÄ±ldÄ±.")
    yield
    # Uygulama kapandÄ±ÄŸÄ±nda:
    # OluÅŸturulan client'Ä± dÃ¼zgÃ¼nce kapat.
    await app.state.httpx_client.aclose()
    logger.info("httpx client kapatÄ±ldÄ±.")

app = FastAPI(
    title="Mihmandar Ä°lim Havuzu API",
    version="3.4",
    description="Tasavvufi eserlerde, makalelerde ve YouTube videolarÄ±nda arama yapma API'si. Analiz geÃ§miÅŸi Turso'da saklanÄ±r.",
    lifespan=lifespan
)
origins = [
    "http://localhost:3000",
    "https://new-git-main-yediulyas-projects.vercel.app",
    "https://mihmandar.org",
    "https://new-mu-self.vercel.app",
    "https://new-yediulyas-projects.vercel.app",
    "https://new.vercel.app",
]
app.add_middleware(CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=["*"], allow_headers=["*"])
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
# PDF dizini - Environment variable'dan al veya varsayÄ±lan kullan
PDF_BASE_URL = os.getenv("PDF_BASE_URL")
PDF_DIR = DATA_DIR / "pdfler"
INDEX_DIR = DATA_DIR / "whoosh_index"
ARTICLES_CACHE = {"data": None, "timestamp": 0}
BOOKS_CACHE = {"data": None, "timestamp": 0}
CACHE_TTL = 3600
YOUTUBE_API_KEYS = [os.getenv(f"YOUTUBE_API_KEY{i}") for i in range(1, 7) if os.getenv(f"YOUTUBE_API_KEY{i}")]
DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
deepseek_client = AsyncOpenAI(base_url="https://api.deepseek.com", api_key=DEEPSEEK_API_KEY) if DEEPSEEK_API_KEY else None
# --- YENÄ°: Dosya sunucu adresini .env'den alacaÄŸÄ±z ---
AUDIO_BASE_URL = os.getenv("AUDIO_BASE_URL") or "https://cdn.mihmandar.org/file/yediulya-ses-arsivi"
PDF_BASE_URL = os.getenv("PDF_BASE_URL")
TURSO_ANALYSIS_URL = os.getenv("TURSO_ANALYSIS_URL")
TURSO_ANALYSIS_TOKEN = os.getenv("TURSO_ANALYSIS_TOKEN")

# Debug: Environment variables'larÄ± logla
print(f"ğŸ” DEBUG - AUDIO_BASE_URL: {AUDIO_BASE_URL}")
print(f"ğŸ” DEBUG - PDF_BASE_URL: {PDF_BASE_URL}")
print(f"ğŸ” DEBUG - TURSO_ANALYSIS_URL: {TURSO_ANALYSIS_URL}")
print(f"ğŸ” DEBUG - TURSO_ANALYSIS_TOKEN: {'***' if TURSO_ANALYSIS_TOKEN else 'None'}")

# Database'i baÅŸlat
init_db()
# --- YardÄ±mcÄ± Fonksiyonlar ---
def get_whoosh_index():
    try: return open_dir(str(INDEX_DIR))
    except Exception as e:
        logger.error(f"Kritik Hata: Whoosh indeksi '{INDEX_DIR}' adresinde bulunamadÄ±: {e}")
        raise HTTPException(status_code=503, detail="Arama servisi ÅŸu anda kullanÄ±lamÄ±yor.")
def get_searcher(ix: Index = Depends(get_whoosh_index)) -> Searcher:
    return ix.searcher()
  
def format_time(seconds: float) -> str:
    minutes, seconds = divmod(int(seconds), 60)
    hours, minutes = divmod(minutes, 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
def extract_video_id(url: str) -> Optional[str]:
    match = re.search(r"(?:v=|\/|embed\/|youtu\.be\/)([a-zA-Z0-9_-]{11})", url)
    return match.group(1) if match else None
def download_audio_sync(url: str, task_id: str) -> bytes:
    temp_dir = tempfile.gettempdir()
    temp_filepath = os.path.join(temp_dir, f"{task_id}_audio.m4a")
    ydl_opts = {'format': 'bestaudio/best', 'outtmpl': temp_filepath, 'quiet': True, 'no_warnings': True, 'noprogress': True}
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download([url])
        with open(temp_filepath, 'rb') as f: return f.read()
    finally:
        if os.path.exists(temp_filepath): os.remove(temp_filepath)
# --- ANA Ä°ÅLEV ---
async def run_video_analysis(task_id: str, url: str):
    try:
        update_task(task_id, "processing", message="Video Bilgileri AlÄ±nÄ±yor...")
        with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True, 'no_warnings': True}) as ydl:
            metadata = await asyncio.to_thread(ydl.extract_info, url, download=False)
          
        update_task(task_id, "processing", message="Video Sesi Ä°ndiriliyor...")
        audio_content = await asyncio.to_thread(download_audio_sync, url, task_id)
      
        update_task(task_id, "processing", message="Ses Metne DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
      
        # deepgram_client = DeepgramClient(DEEPGRAM_API_KEY)  # Åimdilik kaldÄ±rÄ±ldÄ±
        # source = {'buffer': audio_content}
        # options = PrerecordedOptions(model="nova-2", language="tr", smart_format=True, utterances=True)
      
        # response = await deepgram_client.listen.asyncrest.v("1").transcribe_file(source, options)
      
        # if not response.results or not response.results.utterances: raise ValueError("Videodan metin Ã§Ä±karÄ±lamadÄ±.")
      
        update_task(task_id, "processing", message="Konu BaÅŸlÄ±klarÄ± OluÅŸturuluyor...")
        # chapters, chunk_text, start_time = [], "", 0
        # utterances = response.results.utterances
        # for i, utt in enumerate(utterances):
        #     if not chunk_text: start_time = utt.start
        #     chunk_text += utt.transcript + " "
        #     if (utt.end - start_time) >= 120 or (i == len(utterances) - 1 and chunk_text.strip()):
        #         comp_res = await deepseek_client.chat.completions.create(model="deepseek-chat", messages=[{"role": "system", "content": "Verilen metnin ana konusunu Ã¶zetleyen 4-6 kelimelik kÄ±sa bir baÅŸlÄ±k oluÅŸtur."}, {"role": "user", "content": chunk_text}], max_tokens=20)
        #         title = comp_res.choices[0].message.content.strip().replace('"', '')
        #         chapters.append(f"**{format_time(start_time)}** - {title}")
        #         chunk_text = ""
              
        result = {"title": metadata.get("title"), "thumbnail": metadata.get("thumbnail"), "chapters": ["Deepgram ÅŸimdilik devre dÄ±ÅŸÄ± - ses analizi yapÄ±lamÄ±yor."]}
        update_task(task_id, "completed", result=result)
        logger.info(f"[{task_id}] Analiz baÅŸarÄ±yla tamamlandÄ± ve Turso veritabanÄ±na kaydedildi.")
      
    except Exception as e:
        logger.error(f"[{task_id}] GÃ¶rev sÄ±rasÄ±nda HATA oluÅŸtu: {e}", exc_info=True)
        update_task(task_id, "error", message=f"Analiz sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}")
# --- API Endpoints ---
@app.get("/")
async def read_root(): return {"message": "Mihmandar API v3.4 Aktif"}
@app.post("/analyze/start")
async def start_analysis(background_tasks: BackgroundTasks, url: str = Query(..., description="Analiz edilecek YouTube video URL'si")):
    if not DEEPGRAM_API_KEY or not DEEPSEEK_API_KEY:
        raise HTTPException(status_code=503, detail="Video analiz servisi yapÄ±landÄ±rÄ±lmamÄ±ÅŸ.")
      
    video_id = extract_video_id(url)
    if not video_id: raise HTTPException(status_code=400, detail="GeÃ§ersiz YouTube URL'si.")
      
    task_id = video_id
    existing_task = get_task(task_id)
    if existing_task and existing_task['status'] == 'completed':
        return JSONResponse(status_code=200, content={"task_id": task_id, "message": "Bu video daha Ã¶nce analiz edilmiÅŸ.", "result": existing_task['result']})
      
    update_task(task_id, "processing", message="GÃ¶rev BaÅŸlatÄ±lÄ±yor...")
    background_tasks.add_task(run_video_analysis, task_id, url)
    return JSONResponse(status_code=202, content={"task_id": task_id, "message": "Analiz baÅŸlatÄ±ldÄ±."})
# ... DiÄŸer tÃ¼m endpointleriniz ...
@app.get("/search/all")
async def search_all(q: str, authors: Optional[List[str]] = Query(None), searcher: Searcher = Depends(get_searcher)):
    def _search():
        parser = MultifieldParser(["title", "content", "author"], schema=searcher.schema, group=AndGroup)
        query_parts = [f"({q.lower()})"]
        if authors:
            author_filter = " OR ".join([f'author:"{a.lower()}"' for a in authors])
            query_parts.append(f"({author_filter})")
        final_query_str = " AND ".join(query_parts)
        parsed_query = parser.parse(final_query_str)
        results = searcher.search(parsed_query, limit=150)
        formatted_results = []
        for hit in results:
            result_type = hit.get('type')
            if result_type == 'book':
                formatted_results.append({
                    "type": "book", "kitap": hit["title"], "yazar": hit["author"],
                    "sayfa": hit["page_or_id"], "alinti": hit.highlights("content") or hit.get("content", "")[:300],
                    "pdf_dosyasi": hit["source"]
                })
            elif result_type == 'article':
                formatted_results.append({
                    "type": "article", "id": hit["page_or_id"], "baslik": hit["title"],
                    "yazar": hit["author"], "kategori": hit["category"], "url": hit["source"],
                    "alinti": hit.highlights("content") or hit.get("content", "")[:300]
                })
        return {"sonuclar": formatted_results}
    return await asyncio.to_thread(_search)
@app.get("/articles/by-category")
async def list_articles_by_category():
    now = time.time()
    if ARTICLES_CACHE["data"] and (now - ARTICLES_CACHE["timestamp"] < CACHE_TTL):
        return ARTICLES_CACHE["data"]
    data = get_all_articles_by_category()
    ARTICLES_CACHE["data"] = data
    ARTICLES_CACHE["timestamp"] = now
    return data
@app.get("/article/{article_id}")
async def read_article(article_id: int):
    article = get_article_by_id(article_id)
    if not article:
        raise HTTPException(status_code=404, detail="Makale bulunamadÄ±.")
    return article

@app.get("/articles/paginated")
async def get_articles_paginated(
    page: int = Query(1, ge=1, description="Sayfa numarasÄ±"),
    limit: int = Query(12, ge=1, le=50, description="Sayfa baÅŸÄ±na makale sayÄ±sÄ±"),
    search: str = Query("", description="Arama terimi"),
    category: str = Query("", description="Kategori filtresi")
):
    """
    SayfalanmÄ±ÅŸ makale listesi dÃ¶ndÃ¼rÃ¼r
    """
    try:
        from data.articles_db import get_articles_paginated
        
        result = get_articles_paginated(
            page=page,
            limit=limit,
            search_term=search,
            category=category
        )
        
        return {
            "articles": result["articles"],
            "total_articles": result["total"],
            "total_pages": result["total_pages"],
            "current_page": page,
            "per_page": limit,
            "has_next": page < result["total_pages"],
            "has_prev": page > 1
        }
        
    except Exception as e:
        logger.error(f"SayfalanmÄ±ÅŸ makaleler alÄ±nÄ±rken hata: {e}")
        raise HTTPException(status_code=500, detail="Makaleler alÄ±namadÄ±")
@app.get("/authors")
async def get_all_authors(searcher: Searcher = Depends(get_searcher)):
    def _get_authors():
        all_authors = {f['author'].title() for f in searcher.all_stored_fields() if 'author' in f and f['author']}
        return {"authors": sorted(list(all_authors))}
    return await asyncio.to_thread(_get_authors)
@app.get("/books_by_author")
async def get_books_by_author():
    now = time.time()
    if BOOKS_CACHE["data"] and (now - BOOKS_CACHE["timestamp"] < CACHE_TTL):
        return BOOKS_CACHE["data"]
    try:
        with open(DATA_DIR / "book_metadata.json", 'r', encoding='utf-8') as f:
            all_books = json.load(f)
      
        books_by_author_data = {}
        for book_info in all_books:
            author = book_info['author']
            if author not in books_by_author_data:
                books_by_author_data[author] = []
          
            books_by_author_data[author].append({
                "kitap_adi": book_info['book'],
                "pdf_dosyasi": book_info['pdf_file'],
                "toplam_sayfa": book_info['total_pages']
            })
        response_data = {"kutuphane": [
            {"yazar": author, "kitaplar": books} for author, books in sorted(books_by_author_data.items())
        ]}
        BOOKS_CACHE["data"] = response_data
        BOOKS_CACHE["timestamp"] = now
        return response_data
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail="Kitap verileri hazÄ±r deÄŸil.")
    except Exception as e:
        raise HTTPException(status_code=500, detail="Kitap listesi iÅŸlenemedi.")
@app.get("/search/analyses")
async def search_analyses(q: str):
    if not q.strip(): return {"sonuclar": []}
    history = get_all_completed_analyses()
    q_lower = q.lower()
    matching_analyses = []
    for video_id, data in history.items():
        if not isinstance(data, dict): continue
        title = data.get("title", "")
        chapters = data.get("chapters", [])
        chapters_text = " ".join(chapters)
        if q_lower in title.lower() or q_lower in chapters_text.lower():
            matching_analyses.append({
                "video_id": video_id, "title": title,
                "thumbnail": data.get("thumbnail"), "chapters": chapters
            })
    return {"sonuclar": matching_analyses}
@app.get("/pdf/info")
async def get_pdf_info(pdf_file: str):
    try:
        pdf_path = PDF_DIR / urllib.parse.unquote(pdf_file)
        if not pdf_path.is_file(): raise HTTPException(status_code=404, detail="PDF bulunamadÄ±.")
        with fitz.open(pdf_path) as doc: return {"total_pages": len(doc)}
    except Exception as e:
        raise HTTPException(status_code=500, detail="PDF bilgisi alÄ±namadÄ±.")
@app.get("/pdf/page_image")
def get_page_image(pdf_file: str, page_num: int = Query(..., gt=0)):
    try:
        pdf_path = PDF_DIR / urllib.parse.unquote(pdf_file)
        if not pdf_path.is_file(): raise HTTPException(status_code=404, detail=f"PDF bulunamadÄ±.")
        with fitz.open(pdf_path) as doc:
            if not (0 < page_num <= len(doc)): raise HTTPException(status_code=400, detail="GeÃ§ersiz sayfa.")
            page = doc.load_page(page_num - 1)
            pix = page.get_pixmap(dpi=150)
            return StreamingResponse(io.BytesIO(pix.tobytes("png")), media_type="image/png")
    except Exception as e:
        raise HTTPException(status_code=500, detail="Sayfa resmi iÅŸlenirken hata oluÅŸtu.")
@app.get("/search/videos")
async def search_videos(q: str):
    if not YOUTUBE_API_KEYS: raise HTTPException(status_code=503, detail="YouTube servisi yapÄ±landÄ±rÄ±lmamÄ±ÅŸ.")
    all_videos, channels = [], ["UCvhlPtV-1MgZBQPmGjomhsA", "UCfYG6Ij2vIJXXplpottv02Q", "UC0FN4XBgk2Isvv1QmrbFn8w"]
    for channel_id in channels:
        for api_key in YOUTUBE_API_KEYS:
            params = {"part": "snippet", "q": q, "type": "video", "maxResults": 6, "key": api_key, "channelId": channel_id}
            try:
                response = requests.get("https://www.googleapis.com/youtube/v3/search", params=params, timeout=5)
                if response.status_code == 200:
                    for item in response.json().get("items", []):
                        snippet, video_id = item.get("snippet", {}), item.get("id", {}).get("videoId")
                        if snippet and video_id: all_videos.append({"id": video_id, "title": snippet.get("title"), "thumbnail": snippet.get("thumbnails", {}).get("high", {}).get("url"), "channel": snippet.get("channelTitle"), "publishedTime": snippet.get("publishedAt")})
                    break
                elif response.status_code == 403: continue
            except requests.RequestException: break
    return {"sonuclar": all_videos}
@app.get("/analyze/status/{task_id}")
async def get_analysis_status(task_id: str):
    task = get_task(task_id)
    if not task: raise HTTPException(status_code=404, detail="GÃ¶rev bulunamadÄ±.")
    return task
# â˜…â˜…â˜… ANA DEÄÄ°ÅÄ°KLÄ°K BURADA â˜…â˜…â˜…
@app.get("/analysis_history")
async def get_analysis_history(response: Response):
    """
    TÃ¼m tamamlanmÄ±ÅŸ analizleri getirir ve sunucu tarafÄ± Ã¶nbelleklemesini engeller.
    """
    # Bu baÅŸlÄ±klar, Vercel/Railway gibi platformlara bu cevabÄ± asla Ã¶nbelleÄŸe almamasÄ±nÄ± sÃ¶yler.
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
  
    return get_all_completed_analyses()
  
@app.get("/audio/all")
async def list_all_audio_analyses():
    """TÃ¼m analiz edilmiÅŸ ses kayÄ±tlarÄ±nÄ± kaynaÄŸa gÃ¶re gruplayarak listeler."""
    return await asyncio.to_thread(get_all_audio_by_source)
@app.get("/audio/file/{file_name}")
async def stream_audio_from_b2(file_name: str):
    """
    Dosya adÄ±nÄ± alÄ±r, Backblaze URL'sini oluÅŸturur ve sesi stream eder.
    Bu, Railway sunucusu Ã¼zerinden bir proxy gÃ¶revi gÃ¶rÃ¼r.
    """
    if not AUDIO_BASE_URL:
        raise HTTPException(status_code=503, detail="Ses sunucusu yapÄ±landÄ±rÄ±lmamÄ±ÅŸ.")

    # URL encode the filename to handle special characters
    import urllib.parse
    encoded_filename = urllib.parse.quote(file_name, safe='')
    remote_url = f"{AUDIO_BASE_URL.rstrip('/')}/{encoded_filename}"
    
    logger.info(f"Streaming audio from: {remote_url}")
    client: httpx.AsyncClient = app.state.httpx_client

    try:
        # HEAD isteÄŸi ile Ã¶nce dosya bilgilerini alÄ±yoruz
        head_resp = await client.head(remote_url)
        head_resp.raise_for_status()

        headers = {
            "Content-Length": head_resp.headers.get("Content-Length"),
            "Accept-Ranges": "bytes",
            "Content-Type": head_resp.headers.get("Content-Type", "audio/mpeg"),
        }

        async def stream_generator():
            async with client.stream("GET", remote_url) as response:
                async for chunk in response.aiter_bytes():
                    yield chunk
        
        return StreamingResponse(stream_generator(), headers=headers)

    except httpx.HTTPStatusError as e:
        logger.error(f"HTTP Status Error for {remote_url}: {e.response.status_code}")
        if e.response.status_code == 404:
            raise HTTPException(status_code=404, detail=f"Dosya bulunamadÄ±: {file_name}")
        else:
            raise HTTPException(status_code=e.response.status_code, detail="Stream sÄ±rasÄ±nda sunucu hatasÄ±.")
    except Exception as e:
        logger.error(f"Stream hatasÄ± for {file_name}: {e}")
        raise HTTPException(status_code=500, detail="Stream sÄ±rasÄ±nda beklenmedik bir hata oluÅŸtu.")
@app.get("/search/audio")
async def search_audio(q: str):
    """Konu baÅŸlÄ±klarÄ± iÃ§inde metinsel arama yapar."""
    results = await asyncio.to_thread(search_audio_chapters, q)
    return {"sonuclar": results}
# *** BU ENDPOINT'Ä° ESKÄ°SÄ°YLE DEÄÄ°ÅTÄ°RÄ°N ***
@app.get("/audio/stream/{audio_id}")
async def stream_audio_file_by_id(audio_id: int):
    """VeritabanÄ±ndan ID'ye gÃ¶re dosya yolunu bulur ve stream eder."""
  
    # VeritabanÄ±ndan bu ID'ye karÅŸÄ±lÄ±k gelen mp3_path'i al
    relative_path = await asyncio.to_thread(get_audio_path_by_id, audio_id)
  
    if not relative_path:
        raise HTTPException(status_code=404, detail="Ses dosyasÄ± kaydÄ± veritabanÄ±nda bulunamadÄ±.")
    full_path = BASE_DIR / relative_path
    if not full_path.is_file():
        raise HTTPException(status_code=404, detail=f"Ses dosyasÄ± diskte bulunamadÄ±: {relative_path}")
  
    def iterfile():
        with open(full_path, mode="rb") as file_like:
            yield from file_like
    return StreamingResponse(iterfile(), media_type="audio/mpeg")